{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# library\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import * #col, to_date, unix_timestamp\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import shutil\n",
    "from dateutil.relativedelta import relativedelta\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "#from DatabaseModule import DatabaseModule\n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import functions as F\n",
    "#db=DatabaseModule()\n",
    "## library\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import * #col, to_date, unix_timestamp\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import shutil\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "from operator import add\n",
    "\n",
    "## Mias\n",
    "#from pyspark.context import SparkContext\n",
    "#from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col, abs, sum # Solo me traigo tres funciones del paquete functions\n",
    "from pyspark.sql import functions as F # Me traigo todas las funciones del paquete functions\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from numpy import array, ravel\n",
    "#from sklearn.neural_network import MLPRegressor\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "def sqlread(txtsql):\n",
    "    \"\"\"\n",
    "    Carga los datos de una sql\n",
    "    \"\"\"\n",
    "    postgres_host = \"poolpostgresxl.marathon.mesos:5432\"  \n",
    "    postgres_db = \"mutua\"                            # change this with the database you want to connect\n",
    "\n",
    "    user = os.environ.get(\"JPY_USER\")\n",
    "\n",
    "    sslCert = spark.conf.get(\"spark.ssl.datastore.certPem.path\")\n",
    "    sslKey = spark.conf.get(\"spark.ssl.datastore.keyPKCS8.path\")\n",
    "    sslRootCert = spark.conf.get(\"spark.ssl.datastore.caPem.path\")\n",
    "\n",
    "    url = (\"jdbc:postgresql://{host}/{db}?user={user}&ssl=true&sslmode=verify-full&sslcert={sslCert}&sslrootcert={sslRootCert}&sslkey={sslKey}\"\n",
    "                  .format(sslCert=sslCert,\n",
    "                          sslKey=sslKey,\n",
    "                          sslRootCert=sslRootCert,\n",
    "                          user=user,\n",
    "                          host=postgres_host,\n",
    "                          db=postgres_db))\n",
    "\n",
    "\n",
    "    options = {\"driver\":\"org.postgresql.Driver\"}\n",
    "    rst = spark.read.format(\"jdbc\").options(url=url,dbtable=txtsql,properties=options).load()\n",
    "    return rst\n",
    "\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    first, *rest = dfs  \n",
    "    return sqlContext.createDataFrame(\n",
    "        sc.union([df.rdd.repartition(50) for df in dfs]),\n",
    "        first.schema\n",
    "    )\n",
    "\n",
    "def printC(df, active = 1 ):\n",
    "    \"\"\"\n",
    "    Carga los datos de una sql\n",
    "    \"\"\"\n",
    "    if active == 1:\n",
    "        print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------+--------------------+-------------+--------+--------------------+--------------+---+-----------+-----+----+--------------------+\n",
      "|    msid|                 nui|             ip|                  ua|          uaf|     dvf|                uafv|          osfv|  p|        res|   lg|ingh|                  ts|\n",
      "+--------+--------------------+---------------+--------------------+-------------+--------+--------------------+--------------+---+-----------+-----+----+--------------------+\n",
      "|90277284|3976fd55-9d68-47b...|193.146.115.134|Mozilla/5.0 (Wind...|      Firefox|   Other|        Firefox/52.0|    Windows XP|web| '1440x900'|en-US|desc|2017-03-30 00:01:...|\n",
      "|90277303|0179b5a1-b7d3-445...|   88.10.214.45|Mozilla/5.0 (Wind...|       Chrome|   Other|    Chrome/56.0.2924|     Windows 7|web| '1024x640'|   es|desc|2017-03-30 00:03:...|\n",
      "|90277307|c05dd435-1d6a-460...|   71.6.150.144|Mozilla/5.0 (Wind...|      Firefox|   Other|        Firefox/23.0|     Windows 7|web| '1024x768'|en-US|desc|2017-03-30 00:06:...|\n",
      "|90277308|378f3a0d-7c04-4bf...|  87.235.124.33|Mozilla/5.0 (Wind...|       Chrome|   Other|    Chrome/56.0.2924|       Windows|web|'1280x1024'|   es|desc|2017-03-30 00:06:...|\n",
      "|90277321|a4688583-33b5-432...|   80.27.41.253|Mozilla/5.0 (Linu...|Chrome Mobile|SM-G900F|Chrome Mobile/55....|   Android 5.0|web|  '360x640'|   ru|desc|2017-03-30 00:08:...|\n",
      "|90277331|acfa2f13-9f4c-448...|184.169.155.121|Mozilla/5.0 (Maci...|      Firefox|   Other|        Firefox/48.0|Mac OS X 10.10|web| '1024x768'|en-US|desc|2017-03-30 00:11:...|\n",
      "|90277337|9495626b-d022-4bc...|    85.52.9.106|Mozilla/5.0 (Linu...|Chrome Mobile|SM-G930F|Chrome Mobile/56....| Android 6.0.1|web|  '360x640'|   es|desc|2017-03-30 00:13:...|\n",
      "|90277363|12407b51-78e8-4bc...| 46.222.185.172|Mozilla/5.0 (Linu...|Chrome Mobile|SM-G935F|Chrome Mobile/56....|   Android 7.0|web|  '360x640'|   es|desc|2017-03-30 00:13:...|\n",
      "|90277353|28ea8d2a-5f59-4ec...|   71.6.150.144|Mozilla/5.0 (Wind...|      Firefox|   Other|        Firefox/23.0|     Windows 7|web| '1024x768'|en-US|desc|2017-03-30 00:16:...|\n",
      "|90277354|2d980124-3558-4cc...|  217.16.185.66|Mozilla/5.0 (Wind...|      Firefox|   Other|        Firefox/23.0|     Windows 7|web| '1024x768'|en-US|desc|2017-03-30 00:16:...|\n",
      "+--------+--------------------+---------------+--------------------+-------------+--------+--------------------+--------------+---+-----------+-----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlread(\"(\\\n",
    "select msid, nui, ip, ua, uaf, dvf, uafv, osfv, p, res, lg, cast('desc' as text) ingh, ts \\\n",
    "from mutua.autos_vt_main \\\n",
    "limit 10 \\\n",
    ") t\").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlread(\"(\\\n",
    "select idyy, nsol, count(*) nnui \\\n",
    "from \\\n",
    "( \\\n",
    "  select date_part('year', ts) idyy, 'nui' tipo, nui, count(distinct msid) nsol \\\n",
    "  from mutua.autos_vt_main \\\n",
    "  group by date_part('year', ts), nui \\\n",
    "  union all \\\n",
    "  select date_part('year', ts) idyy, 'ip' tipo, ip, count(distinct msid) nsol \\\n",
    "  from mutua.autos_vt_main \\\n",
    "  group by date_part('year', ts), nui \\\n",
    ") t \\\n",
    "group by idyy, nsol \\\n",
    ") t\").toPandas().to_csv(\"res1.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark Python3 - Spark 2.2.0",
   "language": "python",
   "name": "spark2python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
